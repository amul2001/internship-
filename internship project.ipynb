{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cancer Prediction**\n",
    "\n",
    "Dataset Information:\n",
    "\n",
    "Target Variable (y): \n",
    "- Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten features (X) are computed for each cell nucleus:\n",
    "\n",
    "1. radius (mean of distances from center to points on the perimeter)\n",
    "2. texture (standard deviation of gray-scale values)\n",
    "3. perimeter\n",
    "4. area\n",
    "5. smoothness (local variation in radius lengths)\n",
    "6.  compactness (perimeter^2 / area - 1.0)\n",
    "7. concavity (severity of concave portions of the contour)\n",
    "8. concave points (number of concave portions of the contour)\n",
    "9. symmetry\n",
    "10. fractal dimension (coastline approximation - 1)\n",
    "\n",
    "For each characteristic three measures are given:\n",
    "\n",
    "a. Mean\n",
    "\n",
    "b. Standard error\n",
    "\n",
    "c. Largest/ Worst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **import data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cancer = pd.read_csv('https://github.com/YBIFoundation/Dataset/raw/main/Cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-86b5fb7b-99e0-4309-9ed7-e8c91009e45d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86b5fb7b-99e0-4309-9ed7-e8c91009e45d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-86b5fb7b-99e0-4309-9ed7-e8c91009e45d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-86b5fb7b-99e0-4309-9ed7-e8c91009e45d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "cancer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-fb737bca-f188-4205-b4aa-040719d6a725\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb737bca-f188-4205-b4aa-040719d6a725')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-fb737bca-f188-4205-b4aa-040719d6a725 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-fb737bca-f188-4205-b4aa-040719d6a725');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
       "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
       "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
       "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
       "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
       "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
       "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
       "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  ...  texture_worst  perimeter_worst   area_worst  \\\n",
       "count     569.000000  ...     569.000000       569.000000   569.000000   \n",
       "mean        0.181162  ...      25.677223       107.261213   880.583128   \n",
       "std         0.027414  ...       6.146258        33.602542   569.356993   \n",
       "min         0.106000  ...      12.020000        50.410000   185.200000   \n",
       "25%         0.161900  ...      21.080000        84.110000   515.300000   \n",
       "50%         0.179200  ...      25.410000        97.660000   686.500000   \n",
       "75%         0.195700  ...      29.720000       125.400000  1084.000000   \n",
       "max         0.304000  ...      49.540000       251.200000  4254.000000   \n",
       "\n",
       "       smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "count        569.000000         569.000000       569.000000   \n",
       "mean           0.132369           0.254265         0.272188   \n",
       "std            0.022832           0.157336         0.208624   \n",
       "min            0.071170           0.027290         0.000000   \n",
       "25%            0.116600           0.147200         0.114500   \n",
       "50%            0.131300           0.211900         0.226700   \n",
       "75%            0.146000           0.339100         0.382900   \n",
       "max            0.222600           1.058000         1.252000   \n",
       "\n",
       "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
       "count            569.000000      569.000000               569.000000   \n",
       "mean               0.114606        0.290076                 0.083946   \n",
       "std                0.065732        0.061867                 0.018061   \n",
       "min                0.000000        0.156500                 0.055040   \n",
       "25%                0.064930        0.250400                 0.071460   \n",
       "50%                0.099930        0.282200                 0.080040   \n",
       "75%                0.161400        0.317900                 0.092080   \n",
       "max                0.291000        0.663800                 0.207500   \n",
       "\n",
       "       Unnamed: 32  \n",
       "count          0.0  \n",
       "mean           NaN  \n",
       "std            NaN  \n",
       "min            NaN  \n",
       "25%            NaN  \n",
       "50%            NaN  \n",
       "75%            NaN  \n",
       "max            NaN  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cancer.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Target Variable (y) and Feature Variables (X)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cancer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cancer['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer.drop(['id','diagnosis','Unnamed: 32'],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7, random_state=2529)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30), (398,), (171,))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check shape of train and test sample\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=5000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=5000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train or fit model\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-30.20269391])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8644508 , -0.1823121 ,  0.26510852, -0.02688942,  0.13284582,\n",
       "         0.19445151,  0.40918278,  0.20206338,  0.17199488,  0.03798515,\n",
       "         0.0192444 , -1.13284188, -0.13597054,  0.11911954,  0.02266663,\n",
       "        -0.03006638,  0.04691738,  0.02805721,  0.03329433, -0.00980702,\n",
       "        -0.27140621,  0.44034405,  0.16566196,  0.01286379,  0.2719812 ,\n",
       "         0.59704539,  1.06177846,  0.40903862,  0.51193487,  0.08436947]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict model\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', 'M', 'M', 'B', 'M', 'B', 'M', 'B', 'M', 'B', 'B', 'M', 'B',\n",
       "       'M', 'B', 'B', 'M', 'B', 'M', 'B', 'B', 'B', 'B', 'B', 'B', 'M',\n",
       "       'B', 'B', 'M', 'B', 'M', 'B', 'B', 'B', 'B', 'M', 'B', 'B', 'B',\n",
       "       'M', 'M', 'M', 'M', 'M', 'B', 'B', 'M', 'M', 'M', 'B', 'B', 'B',\n",
       "       'B', 'B', 'B', 'B', 'B', 'M', 'M', 'M', 'B', 'M', 'B', 'M', 'M',\n",
       "       'M', 'M', 'B', 'M', 'M', 'B', 'M', 'B', 'M', 'B', 'M', 'B', 'B',\n",
       "       'M', 'M', 'M', 'B', 'B', 'M', 'M', 'M', 'B', 'B', 'B', 'B', 'M',\n",
       "       'B', 'B', 'B', 'M', 'B', 'M', 'B', 'B', 'M', 'B', 'M', 'B', 'B',\n",
       "       'B', 'M', 'B', 'B', 'M', 'B', 'B', 'B', 'M', 'B', 'B', 'B', 'B',\n",
       "       'M', 'B', 'B', 'M', 'B', 'M', 'B', 'M', 'M', 'B', 'B', 'B', 'M',\n",
       "       'M', 'B', 'M', 'M', 'M', 'B', 'B', 'M', 'B', 'M', 'B', 'M', 'B',\n",
       "       'M', 'B', 'M', 'B', 'B', 'M', 'B', 'M', 'M', 'B', 'B', 'B', 'B',\n",
       "       'B', 'M', 'M', 'M', 'M', 'B', 'B', 'B', 'M', 'B', 'M', 'B', 'B',\n",
       "       'B', 'B'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model accuracy\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[97,  5],\n",
       "       [ 2, 67]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9590643274853801"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.98      0.95      0.97       102\n",
      "           M       0.93      0.97      0.95        69\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.96      0.96      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explaination**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract\n",
    "Cancer has identified a diverse condition of several various subtypes. The timely screening and course of treatment of a cancer form is now a requirement in early cancer research because it supports the medical treatment of patients. Many research teams studied the application of ML and Deep Learning methods in the field of biomedicine and bioinformatics in the classification of people with cancer across high- or low-risk categories. These techniques have therefore been used as a model for the development and treatment of cancer. As, it is important that ML instruments are capable of detecting key features from complex datasets. Many of these methods are widely used for the development of predictive models for predicating a cure for cancer, some of the methods are artificial neural networks (ANNs), support vector machine (SVMs) and decision trees (DTs). While we can understand cancer progression with the use of ML methods, an adequate validity level is needed to take these methods into consideration in clinical practice every day.\n",
    "\n",
    "In this study, the ML & DL approaches used in cancer progression modeling are reviewed. The predictions addressed are mostly linked to specific ML, input, and data samples supervision.\n",
    "\n",
    "1. Introduction\n",
    "The main weight of ailment overall is as Lung malignancy that is the most inescapable disease in the two men and women [1]. A few other reports estimate some 221,200 new cases of pulmonary cancer occur and represent approximately 13% of all cancer diagnoses in 2015. Approximately 27 percent of all cancer deaths are attributed to lung cancer [2]. Lung nodules must therefore be closely examined and monitored when at an early stage. In this study, the ML & DL approaches used in cancer progression modeling are reviewed. The predictive models discussed here are based on different supervised ML techniques, input and data samples.\n",
    "\n",
    "A Local Binary Pattern (LBP) is an image operator that transforms an image into an array or picture of integer labels that describe the appearance of the small picture. These labels are then used for further image analysis, most frequently in the histogram. The LBP texture operator has become a popular approach to various applications thanks to its discriminative power and computational simplification [2].\n",
    "\n",
    "A Binary Local Pattern (LBP) is a picture administrator that changes over a picture into a variety of number names speaking to its essence. These markers are then more commonly used in the histogram for further image processing. In the last three decadent years the prevalence of prostate and breast cancer in male and female cancer has been the largest, but lung cancer remains the highest in cancer-patient mortality [3]. One of the main reasons for this is that prostate and breast cancer prognostic models are comparatively more advanced and systemic than pulmonary cancer. Thus, it is urgently necessary to establish an effective early-stage lung cancer forecast model. In linear and non-linear problems, SVM has superior predictor performance and is widely used in various fields including in medical matters. Even if SVM is a superior classifier, the field of cancer prognosis models is relatively immature [4].\n",
    "\n",
    "The mutation test [5] has become an important tool for deciding the right therapy options for patients in clinical tests. Direct sequencing is an alternative approach for unknown mutations based on screening. The Mutation Test for Epidermal Growth Factor Receptors (EGFR) has been identified for lung cancer genetic mutation testing [4]. A contrast with their non-ensemble variants of two types of categorizing equipment Artificial Neural Network (ANN) and Support Vector Machine (SVM) is published. The weight of the misjudgment for the majority is higher than that of the minority class and is likely to cause misjudgment. Traditional algorithms of classification are not successful and excellent.\n",
    "\n",
    "1.1. Topology of machine learning & deep learning algorithms\n",
    "In order to predict the various types of diseases, different deep learning & machine learning algorithms are used , such as Support vector machine (SVM), Neural Network (NN), LR, Nevin biases (NB), Fuzzy logic, transfer learning, ensemble learning, Transduction learning, KNN, and Adaboost are mostly utilized in diverse contributions. Moreover, SVM is categorized into Boosted SVM & MLSVM for predicting distinct diseases in the earlier contributions. Similarly, NN is classified as Dynamic Neural Network (DNN) & Convolution Neural Network (CNN) which are employed for diagnosing different diseases in different contributions. Moreover, GBDT is a modified form of DT, CVIFLR is the modified form of LR that are used for detecting diseases. Moreover, RF and Fuzzy logic is grouped into HRFLM and Fuzzy SVM, respectively in order to predict discrete diseases in various contributions. So, for predicting lung cancer in an efficient manner with the help of improved machine learning techniques can be use.\n",
    "\n",
    "2. Literature review\n",
    "ChaoTan et al [1] explored the feasibility of using decision stumps as a poor classification method and track element analysis to predict timely lung cancer in a combination of Adaboost (machine learning ensemble). For the illustration, a cancer dataset was used which identified 9 trace elements in 122 urine samples. The sample set partitioning was performed using Kennard and Stone algorithm (KS), combined with alternative samples. The adaboost forecast results were contrasted with the Fisher Biased Analytic (FDA) results. In the test set, 100% of Adaboost's sensitivity for both cases was reached, 93.8% of accuracy was 95.7% and 95.1% respectively for case A and case B 96.7%. The structure of both the test data is less reactive than the FDA and the change is often easier to monitor than the FDA. The Adaboost appeared superior to FDA and proved that combining Adaboost and urine analysis could be a valuable method through clinical practice for the diagnosis of early lung cancer.\n",
    "\n",
    "Tae-WooKim et al. [2], have developed a decision tree on occupational lung cancer. In 1992–2007, 153 lung cancer cases were reported by the Occupational Safety and Health Researcher's Institute (OSHRI). The objective parameter was to determine if the situation was accepted as lung cancer linked to age, sex, smoking years, histology, industry size, delay, working time and exposure of independent variables. During the whole journey for indicators for word related cellular breakdown in the lungs the characterization and relapse test (CART) worldview is utilized. Presentation to known lungs disease specialists was the best pointer of the CART model. As the CART model is not absolute, the functionality of lung cancer must be carefully determined.\n",
    "\n",
    "Maciej Zięba et al. [3] introduced boosted SVM in 2014 which is dedicated to solving imbalanced results. The solution proposed combined the advantages of using ensemble classifiers with cost-sensitive support vectors for uneven data. In addition, a method for extracting decisions from the boosted SVM was presented. In the next step, the efficiency of the solution proposed was assessed by comparing the performance of the unbalanced data with other algorithms. Finally, improved SVM was used to estimate after surgery life expectancy in patients with lung cancer.\n",
    "\n",
    "A multiclass data pathway behavior transformation approach called Analysis-of-Variance Based Feature Set (AFS) was suggested by Worrawat Engchuan [4]. The results of the classification using pathway behavior derived from the proposed approach indicate that all four lung cancer data sets used have high classification capacity in three-fold validity and robustness.\n",
    "\n",
    "H. Azzawi et al. [5] proposed a GEP (gene expression) model to forecast microarray data on lung cancer in 2016. In order to extract important lung cancer related genes, the authors use two approaches for selecting genes and thus suggest specific GEP prediction models. The validation of the cross-data collection was tested for reliability. The test results show that, considering precision, sensitivity, speciality, and region under the recipient functional property curve, the GEP model using fewer features surpassed other models. The GEP model was a better approach to problems of diagnosis of lung cancer. It has been found.\n",
    "\n",
    "Panayiotis Petousis et al. [6] created and evaluated a range of dynamic Bayesian Networks (DBN) to assist in informing decisions about lung cancer screening by providing insights into how longitudinal data can be used. The NLST dataset LDCT arm has been used in creating and exploration five DBNs for high-risk people. 3 of the DBNs were designed with a reverse style, and 2 through methods of structural learning. All applications are based on population, smoking status, a history of cancer, family history of lung cancer, risk factors for exposure, lung cancer co-orbidities and information on LDCT screenings. In view of the uncertainty resulting from lung cancer screening, a lung cancer-state model was used to identify the individual's cancer status over time. These models have been tested on balanced cancer and non-cancer research and test sets in order to resolve data disequilibrium and over fitting. Expert judgments contrasted the results. In all three NLST test intervention stages, the average area underneath the curve (AUC) of the receiver operating feature (ROC) was above 0.75. Superior were compared models such as logistic regression and naïve Bay. Lung screening DBNs have demonstrated strong discrimination and predictive strength in both cancer and non-cancer cases.\n",
    "\n",
    "The SEER database was used by Chip M. Lynch et al. [7] to classify the survival of lung cancer patients as a linear regression, decision trees, gradient boosting machines (GBM), support-vector machines (SVMs) and a custom set. In order to allow the comparisons between the different approaches, the main data attributes for applying these processes includes the tumor level, tumor size, gender, age, stage and number of primaries. Rather of being divided into classes, the prediction has been viewed as a continuous goal as a first step to enhancing survival. Results have indicated that the expected values conform to the actual values, which constitute the majority of the results, for low to moderate survival. The model that was most popular in the custom set was GBM, though Decision Trees did not function, because it consists of some discreet performance. The outcome show that GBM with RMSE value of 15.32 was the most precise of the five individual models produced. While the SVM has an underperformed RMSE of 15.82, the SVM is perhaps the only system delivering a distinctive efficiency in the quantitative tests. The results of the simulations were consistent with a traditional Cox proportional risk model, which is used as a reference point. In order to inform the patient's decision in final analysis of these supervised learning strategies, SEER data were found to be used as a way of assessing the time for patient survival and that the findings of these technologies for this particular dataset may equate to those of conventional methods.\n",
    "\n",
    "Deep learning is dependent on several covered layers on use of neural “deep” networks, which have understood relations between the top two strata and its down bound ties to all their lower characteristics were tested to determine the malignancy of its Lung nodules despite the computation of the morphology and structure characteristics [9]. For a strong faith network, they had hit the responsiveness rate of 73.40% and the specificity rate of 82.20%.\n",
    "\n",
    "Deep Convolutional Neural Network CNNs is used to identify or label a medical image in some research papers. Diagnosed lung cancer in 2015 with a multiscal two-layer CNN [13] recorded 86.84% accuracy in [12] the CNN architecture, data set characteristics, and transfer learning factors were exploiting and extensively analyzing three significant and previously under studied factors.\n",
    "\n",
    "Predictive methods for breast cancer's survival by a large dataset were built in [15] by the computational regression of 2 major data mining methods, artificial neural networks and Decision Trees. The impartial approximation of the three prediction models was measured by ten times the cross-validation methods for comparative analysis. Results indicate that the Decision Tree (C5), second most effective artificial 91.2 percent neural networks, and the 89.2 percent logistic regression models, are the best predictor of 93,6 percent accuracy for the holdout study. A study was conducted with predictive models for the survival of prostate cancer, using vector support machines (SVM) in relation to the three techniques.\n",
    "\n",
    "In [16] SVM with Artificial Neural Networks and Decision-making Trees is identified in this case as the precision predictor (92,85% accuracy). Prostate cancer survival is also examined in context, including artificial neural networks, decision trees and logistical regression. In the segment, data on patients suffering from colon cancer were compared to predict survival and more accurate neural networks were determined.\n",
    "\n",
    "In [19] the assembly of the 3 most effective classification methods leads to an ideal forecast and region under the ROC for colon cancer survival. Some studies have evaluated the survival of lung cancer patients by analyzing the SEER database using learning machinery, such as group class-based methods and SVM and logistic regression. Techniques to assess the probability of development of lung cancer in patients with certain symptoms have been analyzed in data classification techniques. Comparisons with the lung cancer data in were made with the C4.5 and Naïve-Bayes graders and 90% of the survival estimates were achieved. In [19], [20], there was the establishment of a joint voting process with five Decisions to provide the best predictor of the precision and survival area of ROC lung cancer. In order to identify interesting association rules or correlation among a wide range of items, Association rules mining techniques have been used; different methods for extracting rules and standard criteria have been proposed to indicate the best way to choose the rules and optimize them based on a specific data set.\n",
    "\n",
    "A pulmonary cancer rulebook was developed using automated technology in, some of which was redundant and manually removed on the basis of domain expertise. There were three factors: maximum branch factor, addition of a new branch, and the factor used to add a new branch. From the very beginning, the authors suggested a tree algorithm that uses the whole dataset and descends in depth to the data with a greedy approach. Each tree node was a segment and therefore a rule of association. The attributes were: age, birthplace, grade of cancer, diagnostic test confirmations, the most remote tumor extension, involvement with lymph nodes; type of operation performed, cause for no operation; order of operation and radiation; area of the lymphatic node surgery, cancer phase. The measurement of treatment effectiveness and surgery is a required result of a SEER dataset analysis, despite a lack of chemotherapy information in the dataset.\n",
    "\n",
    "Treatment effectiveness was taken into account in [21]. The study investigated whether patients with lung cancer had survival or radiation for longer or for both. A Propensity Score was utilized which represents a dependent likelihood of treatment for a unit given a collection of covariates observed. Two methods were used, known as logistic regression and classification tree, for the assessment of score. As patients can be treated separately or together for surgery or radiation, the score for every class was calculated and the variables were then numbered. A mathematical collection was generated on the life expectancy and radiation combination, together with a grading tree to every cluster. The results demonstrated that consumers that didn't obtain radiation both with and without surgery have the longest survival time.\n",
    "\n",
    "2.1. Analysis of ML applicability’s in cancer\n",
    "A comprehensive search of ML techniques in cancer sensitivity, recurrence and survival predictions was conducted. The PubMed, Scopus, has entered two online databases. Further review was required due to the large no of blogs which were found by the search queries Most of the studies use different input data types: Clinical, genetic, histological, imaging, socioeconomic, epidemiological or mixed details. According to [13] and their survey based on ML use of cancer prediction, we have seen a large rise in documents released in the last decade. Discuss, we selected from the first category of papers a representative list following a well-defined structure. We have selected such studies in order to prevent the desired effects, especially with the implementation of recognisable ML techniques and integrated data through heterogeneous data. Tables 1 indicate some publications in this study. Each suggested approach specifies the type of cancer, the ML treatment, number of patients, type of data and overall precision achieved. Each table corresponds to a particular scenario of study. i.e., prediction of cancer sensitivity, forecast of cancer occurrence and prediction of cancer survival. It should have been acknowledged to introduce the most precise predictive model here in publications which apply more than one ML technique to prediction. Different study projects have attempted to forecast cancer regeneration after remission and have managed to boost predictions correctly in contrast to alternative statistical techniques. In addition, molecular and clinical data have been used to estimate the large bulk of such papers. The implementation of observed behaviors such as input data is a growing phenomenon, based on the growth of HTTs.\n",
    "\n",
    "Table 1. Features and challenges of existing lung cancer prediction models.\n",
    "\n",
    "Author [citation]\tMethodology\tFeatures\tChallenges\n",
    "Tan et al. [1]\tAdaboost\t\n",
    "•\n",
    "Has attained high sensitivity and best performance.\n",
    "\n",
    "•\n",
    "It is very simple to implement.\n",
    "\n",
    "•\n",
    "It is very sensitive to noisy data.\n",
    "\n",
    "Tae-WooKim et al. [2]\tDecision Tree (DT)\t\n",
    "•\n",
    "These are simple to interpret.\n",
    "\n",
    "•\n",
    "It should be taken as the minimal decision standard of work-relatedness for lung cancer.\n",
    "\n",
    "•\n",
    "They suffer from over fitting.\n",
    "\n",
    "Maciej Zięba et al. [3]\tBoosted SVM\t\n",
    "•\n",
    "It is used in medical application for predicting post-operative life expectancy in lung cancer patients.\n",
    "\n",
    "•\n",
    "It is used to solve the imbalanced data problems.\n",
    "\n",
    "•\n",
    "The running time of training algorithms do not scale well with the size of the training set.\n",
    "\n",
    "Worrawat Engchuan [4]\tSVM\t\n",
    "•\n",
    "It is used to build n- hyperlanes and n-features for dividing each different class apart from maximal margin.\n",
    "\n",
    "•\n",
    "Many parameters need to be set accurately for attaining the best results.\n",
    "\n",
    "H. Azzawi et al. [5]\tGene Expression Programming\tGene Expression Programming\t\n",
    "•\n",
    "It has better solution for predicting lung cancer difficulties.\n",
    "\n",
    "•\n",
    "Has high accuracy.\n",
    "\n",
    "Panayiotis Petousis et al. [6]\tDynamic Bayesian Networks\tDynamic Bayesian Networks\t\n",
    "•\n",
    "Has demonstrated high discrimination and predictive power.\n",
    "\n",
    "•\n",
    "It is used to acquire the probability of positive outcome of a biopsy for the given individual.\n",
    "\n",
    "Chip M. Lynch et al. [7]\tDT\t\n",
    "•\n",
    "It is the best predictor by attaining high accuracy.\n",
    "\n",
    "•\n",
    "It automatically prunes to a very short three-level depth.\n",
    "\n",
    "P. Petousis et al. [10]\tPartially- Observable Markov Decision Process (POMDP)\tPartially-Observable Markov Decision Process (POMDP)\t\n",
    "•\n",
    "It optimizes the lung cancer prediction during the improvement of test specificity.\n",
    "\n",
    "•\n",
    "It reduces the false positive rates.\n",
    "\n",
    "2.2. Case study 1\n",
    "Application of machine learning to predict the susceptibility of cancer risk from the 79 papers surveyed in this study are relative limited (only 3). The development of a retrospective methodology to predict the presence of 'spontaneous' breast cancer using single nucleotide polymorphism (SNP) steroid metabolizing enzymes (CYP 450) is among the interesting documents. Close. Sporadic and non-family breast cancers account for 90% (Dumitrescu and Cotarla, 2015). This trial was conducted with the theory that environmental toxins or hormones were accumulated in breast tissue and that some combinations of the SNP gene were at increased risk of breast cancer. The authors have obtained data on 63 breast cancer patients and 74 breast-free (controls) patients from the SNP (98 SNP from 45 cancer-associated Genes). It was vital to the progress of this research that researches used various methods to minimize a sample-per-feature ratio and analyzed several processes of machine training in order to find optimum classification. In particular, the authors rapidly reduced this set from a start set of 98 SNPs to just 2–3 SNPs that appeared to be as informational as possible. Instead of almost 3:2 (with all the 98 SNPs used), the specimen ratios were reduced to 45:1 (for 3 SNPs) and 68:1 (for 2 SNPs). This made it possible to prevent the “dimensionality curse” from being affected (Bellman, 1961; Somorjai et al., 2013). When the testing sample gets minimized, a number of machine learning techniques, consisting of a naïve Bayes model, various decision-making methods and a sophisticated SVM were applied. With just a set of three SNPs the SVM and naïve Bayes classifier were maximum in precision. The decision-tab classifier achieved maximum accuracy with a set of two SNPs. The SVM classification was the optimum, along with a precision of 69%, and 67% and 68%, respectively, were found in the naive Bayes and Decision Tree classification systems. The outputs are about 23–25% better than original. The extensive level of cross validation and confirmation conducted was another notable feature of this study. At least three ways have been validated for each model's predictive power. Firstly, model training with 20-fold cross validation has been evaluated and monitored. A bootstrap resampling approach was used when the cross validation is performed 5 times and the outputs were averaged to keep the stochastic dimension in the division of samples to a minimum. In addition, the selection process was carried out for 100 times in each fold (5 times for each of 20 folds) in order to reduce unequality in function selection (i.e. selecting the most informative SNP sub-ensemble). Thus, the outputs are then matched with an altered permutation test that, had 50 percent predictive precision. While the researchers tried to reduce the stochastics in sample partitioning, it could have been better to use leave-one-out cross-validation that shall have completely deleted this stochastical element. This trial was conducted with the theory that environmental toxins or hormones were accumulated in breast tissue and that some combinations of the SNP gene were at increased risk of breast cancer. The authors have obtained data on 63 breast cancer patients and 74 breast-free (controls) patients from the SNP (98 SNP from 45 cancer-associated Genes). It was vital to the progress of this research that researchers used various models to minimize a sample-per-feature ratio and analyzed several methods of machine training in order to find optimum classification. It also points out the wayin which machine learning can disclose significant information into the biology of spontaneous or non-famile breast cancer and polygenic risk factors.\n",
    "\n",
    "2.3. Case study 2\n",
    "Cancer Survival Prediction Almost half (or 1 year or 5 years survival rates) of all the machine study studies on cancer prediction. One report of a specific interest (Futschik et al., 2013) was used to predict the outcomes of diffused large-B-cell lymphoma (DLBCL) by hybrid machine learning. In particular, both clinical and genomic (microarray) information was gathered into creating one clinical classification to predict DLBCL survival. The approach is somewhat different from the Listgarten et al. study (2014) in its classification scheme, which only used genomic data (SNP). Futschik et al. hypothesized, rightly, that clinical knowledge may improve data on microarrays to a better output than a microarray-alone or clinical data-based classifier. In addition, different kinds of “Evolving Neural Network” (EFuNN) classifiers have been produced to manage genomic data, separate from the Bayesian classification system. A mixture of 17 genes from the microarray data is used by the best EFuNN classifier. The accuracy of this best EFuNN was 78.5%. In order to achieve consensus prediction, the EFuNN and the Bayesian classificators were mixed in a hierarchic modular structure. This hybrid classifier has a precision of 87.5 per cent, significantly improving both classifications' performance alone. This was 10% good than the excellent-performing classification for machine learning (77.6% by VMS). A cross-validation strategy for the EFuNN classifier was applied. Possibly because the sample was small. No external validation collection was present to check for the overall system, as with Case Study # 1. The Sample per Feature Ratio (SFR) is above 3 with just 56 patients (samples) categorized using 17 gene features. SFR below 5 do not always necessarily ensure a robust classification (Somorjai et al., 2013). Moreover, it is obvious that the researchers were known of this problem and spent enough time explaining in depth the internal functioning of their classifier to justify their approach. This consisted of a summary of how the Bayesian classification was constructed, how the EFuNN works and how the two classification systems cooperate to make one prediction. Also, the researchers tested the independence of the micro-array knowledge from clinical data and subsequently verified it. This eye for detail is particularly outstanding in such a study. The whole research reveals how well the capacity to use both genomic and clinical data significantly improves cancer prediction accuracy.\n",
    "\n",
    "2.4. Case study 3\n",
    "The Laurentian and the other in a particularly good example, and also discusses some of the inconveniences observed in existing researches. The authors wanted to predict the possibility of recurrence in patients with breast cancer for five years. Seven predictive variables have been combined, comprising of clinical information like patient age, tumor size and no. of axilla metastases. Protein biomarkers, like oestrogen and progesterone receptor levels, also received information. The focus of the research was to produce an automated, quantitative predictive approach more precise than those of the classical metastasization of the tumor node (TNM). TNM is a group of medical experts that rely majorly on the professional judgment of a pathologist or clinician. The researchers used an ANN model, were using information from 2441 breast cancer patients (each time seven data points). A sample to feature ratio remained significantly higher than the recommended minimum of five (Somorjai et al., 2013). The whole dataset was divided into 3 classes: training (1/3), testing (1/3) and test sets (1/3). Furthermore, the authors have collected 310 separate samples from another organization to carry out an external assessment of breast cancer patients. This helped the researchers to test the generalization of their system out beyond ones institution — a phase not taken in the 2 experiments discussed above. The analysis demonstrates not only the volume of data and the thoroughness of validation, but also the level of quality control for data processing. The information, for example, was decided to enter and collected autonomously in a connection database and was autonomously checked to keep the referring doctors in good standing. The samples of 2441 patients and 17 000 data points were sufficiently large for a typical breast cancer population demographics when subdivided into the data sequence. However, by examining data distributions for patients in each set (training, monitoring, testing and external), the authors explicitly verified this assumption and demonstrated that distributions are quite same. The Authors built an extremely accurate and robust classifier through consistency and attention to detail Since the study's aim is to produce a system that better predicted re-currence of breast cancer than the traditional TNM stalemate method, comparing the ANN model with the TNM stalemate predictions was important. This was done by using an Operator Characteristic (ROC) curve to compare the performance. The ANN model (0.726), calculated by the portion in the ROC curve, exceeded the TNM system (0.677). This research is an brilliant illustration that machinery is well articulated and tested. A large enough set of data was obtained and data was tested for performance assurance and precision for each sample independently. In addition, blinded validation sets were available for assessing the generality of the machine learning system both from the original data set and through an external point. Finally, the precision of the model has been contrasted directly with that of the traditional TNM projection scheme. Thus the only challenge to this analysis was that the researchers evaluated only one form of ANN algorithm. Because of the type and the amount of data used, another machine learning technique can well have exceeded their ANN model.\n",
    "\n",
    "3. Research gap\n",
    "Lung cancer is the second largest human illness, which refers to deaths from cancer worldwide. The average survival rate of 5 years for patients with lung cancer in other organs such as the breast, cervix, bladder, prostate or colon does not exceed 14 percent, which is significantly less than the rate of patients with cancer [18]. Thus, early prediction of lung cancer is very important for the appropriate treatments for decreasing the deaths. In big data, healthcare is one of the significant sources. Accurate examination of healthcare information is mostly demanded for detecting lung cancer in an early stage. Multiple researches are being designing newly to recognize lung cancer with more quality using big data. As there is a necessity to classification approach for improving the detection accuracy with respect to time. In addition, machine learning techniques are modelled for enhancing the detection accuracy in big data. Specifically, lung cancer is not well known that means which kind of approaches will give high detection data and which data attributes must be employed for the detection purpose.\n",
    "\n",
    "With the help of huge datasets, prediction methods for breast cancer survivability were introduced by implementing two famous data mining techniques such as Artificial Neural Network (ANN) and Decision Tree (DT) and also utilized a common statistical approach. For measuring the unbiased assessment of three detection models, ten-fold cross validation mechanisms were used for the performance comparison. The outcomes have proven that DT was the well performing classifier for predicting the disease with an accuracy of 93.6% on the holdout sample; ANN was standing the second best position with an accuracy of 91.2%. Similarly, logistic regression has attained the accuracy of 89.2%. A research was done in [20] for developing detection techniques to know the survivability of prostate cancer, using SVM along with that three methods that were mentioned earlier. Here, the outputs have revealed that the singled out SVM acquired high accuracy, next to that ANN and DT attained more accuracy. Moreover, prostate cancer survivability was examined by ANNs, DTs, and LR methods in [21]. Multiple methods were contrasted in [22] by SEER colon cancer patient dataset for predicting survival rate, and recognized that NNs were best for predicting the survival rate. Ensemble voting of three outperformed classifiers present in [23] was resulted in optimal prediction, and AU-ROC curve to colon cancer survival rate. In some researches, the survival of lung cancer patient was examined by evaluating the SEER database using machine learning algorithms, consisting of SVM, LR [24], unsupervised approaches [25], and clustering-based techniques [26]. In [27], data classification approaches were assessed for finding the chances of patients with definite indications for the growth of lung cancer. The performance of DT and NB classifiers were compared in [28], and implemented for lung cancer data acquired from SEER database. This attained approximately 90% precision in detecting the survival of patient. Ensemble voting of five DTs and meta-classifiers existing in [29] was resolute for acquiring the best prediction survival rate of lung cancer regarding precision and AU-ROC curve. Many challenges related to the machine learning algorithms are associated with manual training. The significant thing is complexity in accurate recognition of nature for pre- processing them correspondingly before subjected to machine learning algorithms. The time and the experts linked with this job were majorly high. According to the research, it was manifested that there is lack of consistency in detection accuracy of machine learning techniques over classical prediction techniques. With the present literature, this was made reliable. Many investigations that compared the machine learning models with classic statistical model have been confirmed that their outcomes were different.\n",
    "\n",
    "Even though multiple strategies were utilized for predicting different types of diseases, the predictive models using the machine learning algorithms reported in the literal works are less for lung cancer detection with IoT integration. There is a high scope to implement more well- performing deep learning models that might produce best prediction outcome. Moreover, the enlarged availability of adequate historical data of patients has paved the way for the development of novel deep learning algorithms for lung cancer prediction. In addition, the optimization algorithms have the ability to improvise the deep learning models. However, there are few disadvantages such as it is not able to find the optimal solution to the problem defined, and it is complex to select parameters. Moreover, the benefit of PSO is its ability to solve the complex optimization problem. But, the convergence concept is not applicable. Some of the positives of SMO are useful for solving quadratic problems that occurs in the training of SVM, and also it reduces the memory storage. Yet it has to improve by introducing a new variant. The ability of machine learning to solve composite tasks with dynamic environment and knowledge has contributed to its success in prediction research especially lung cancer, enabled with novel met-heuristic algorithms.\n",
    "\n",
    "Although there are many advantages for predicting the lung cancer, but still there are few defects with the existing methodologies so that a new method needs to be implemented. Adaboost [31] has attained high sensitivity and best performance, and it is very simple to implement. But, it is very sensitive to noisy data. DT [32] is simple to interpret, it should be taken as the minimal decision standard of work-relatedness for lung cancer, is the best predictor by attaining high accuracy, and it automatically prunes to a very short three-level depth. However, the running time of training algorithms do not scale well with the size of the training set. SVM [34] is used to build n-hyperlanes and n-features for dividing each different class apart from maximal margin, and it improves the classification power and robustness. Yet, many parameters need to be set accurately for attaining the best results. Gene Expression Programming [35] has the better solution for predicting lung cancer difficulties, and has high accuracy. However there are some disadvantages such as if they are easy to manipulate, they lose in functional complexity. Dynamic Bayesian Networks [36] has demonstrated high discrimination and predictive power, and it is used to acquire the probability of positive outcome of a biopsy for the given individual. Though there are few challenges like if there is longer search time, the performance might be affected. POMDP [38] optimizes the lung cancer prediction during the improvement of test specificity, and it reduces the false positive rates. But, the performance needs to be improved. Hence, the new model needs to be introduced for providing best performance so that the above conflicts are useful for the new development method.\n",
    "\n",
    "4. Research objectives\n",
    "The objective of this research work is discussed as follows.\n",
    "1.\n",
    "To review on various state-of-the-art lung cancer prediction models and develop a new feature extraction model.\n",
    "\n",
    "2.\n",
    "Compare the symptoms of cancer for early notification.\n",
    "\n",
    "3.\n",
    "To design and develop a deep learning model to predict the lung cancer.\n",
    "\n",
    "4.\n",
    "To validate the proposed model by comparing it with other conventional models.\n",
    "\n",
    "5.\n",
    "Sending Automatic notification for detecting the cancer.\n",
    "\n",
    "\n",
    "5. Discussion\n",
    "In The latest research on predicting cancer using ML & DL techniques are discussed in this study. Further through the short details of the ML & DL field and the preprocessing data techniques, the selection techniques and the classification algorithms were employed, we discussed three specific case studies based on popular ML tools, concerning foretell of the susceptibility of cancer, cancer recurrence and cancer survival. Clearly, a huge number of ML & DL concepts released over the past decade produce precise outputs regarding particular cancer predictions. Moreover, it is crucial for the separation of clinical decisions to identify potential problems including experimental design, collecting suitable samples of data and validating classified results. Moreover, despite claims to have contributed to appropriate and efficient decision-making by the ML classification methods, very few have in fact entered clinical practice. Recent advances in omits technology have led us further to better understand a wide range of diseases, but validation results need to be accurate before signatures of gene expression shall be used in hospitals. Only a few marked samples in general. The small amount of data samples is a majorly frequent drawback observed in the research surveyed in this article. The size of training data sets that need to be large enough is a basic requirement in the use of classification schemes to model a disease. A relatively large dataset makes it possible to divide enough into training and trial sets and therefore to validate the calculators reasonably. A small training sample can result in misclassifications compared with the dimension of the data, while estimators can develop unstable and partial techniques. It’s clear that a more wealthy group of patients could predict their survival may improve predictive model capacity. The quality of the dataset and the selection schemes are important for efficient ML and DL and then for precise cancer foretell except for data size. Using feature selection methods to select the maximum informative characteristics subset for training the technique could lead to sturdy models. Reproducible values are also characterized as characteristic sets consisting of histology and pathology studies. Given the lack of static entities, it is essential that a multiple feature sets are adapted to the ML & DL technology over time. We also discovered which SVM and ANN classifiers are commonly utilized for cancer forecasting results as one of the most frequently used ML algorithms [35]. As discussed in our introductory section, ANNs are widely used for nearly 30 years [40]. SVMs are also a newer method to cancer prediction but have already been widely included in their trustworthy predictive results. However, the selection of the best algorithm is dependent on a large number of parameters, which include data types collected, sample size, time limits and the type of prediction results. New methods for overcoming the above-mentioned limitations should be explored regarding the future of cancer modeling. More accurate results and reasoned conclusions would be obtained through efficient quantitative research of the heterogeneous data sages used. Further research on the basis of more public databases, which gather valid cancer data for all diagnosed patients, is needed. Their use by scholars will allow their modeling studies to generate relevant outputs and integrated clinical decision-making.\n",
    "\n",
    "6. Conclusion\n",
    "The whole study explains and compares the findings of various machine learning and in-depth learning implemented to cancer prognosis. Specifically, several trends related to those same kinds of machines techniques to be used, the kinds of training data to be incorporated, the kind of endpoint forecasts to be made, sorts of cancers being investigated, and the overall performance of cancer prediction or outcome methods have been identified. While the ANNs are common, it is clear that a broader variety of alternative learning approaches is also used to predict at least three different cancer types. ANNs continue to be prevalent. Furthermore, it is clear that machine training methods typically increase the efficiency or predictable accuracy of most pronostics, in particular when matched with conventional statistical or expert systems. Although most researches are usually excellently-designed and fairly validated, more focus is quite desirable for the planning and implementation of experiments, in particular with regard to quantity and quality of biological data. Improving the experimental design and the biological validation of several device classification systems would undoubtedly increase the general Quality, replicability and reproductivity of many systems. In total, we believe that the usage of the devices education & deep learning classificatory will probably be quite common in many clinical and hospital settings if the quality of study continues to improve.\n",
    "\n",
    "The assimilation of multifaceted heterogeneous data, which can offer a promising tool for cancer infection and foresee the disease, also demonstrates the incorporation in the application of different analytical and classification methods.\n",
    "\n",
    "In future, by using the proposed framework, we would like to use other state of the art machine learning algorithms and extraction methods to allow more intensive comparative analysis.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
